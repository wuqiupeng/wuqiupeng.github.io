{"meta":{"title":"JAVA码农随笔","subtitle":null,"description":null,"author":"wuqiupeng","url":"https://www.wqp0010.top"},"pages":[{"title":"","date":"2020-06-30T00:33:58.866Z","updated":"2020-06-30T00:33:58.866Z","comments":false,"path":"categories/index.html","permalink":"https://www.wqp0010.top/categories/index.html","excerpt":"","text":""},{"title":"","date":"2020-06-30T00:33:58.866Z","updated":"2020-06-30T00:33:58.866Z","comments":false,"path":"tags/index.html","permalink":"https://www.wqp0010.top/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Kafka深度解析 - Kafka Stream","slug":"Kafka深度解析-Kafka-Stream","date":"2019-03-30T07:40:00.000Z","updated":"2020-06-30T07:09:36.384Z","comments":true,"path":"2019/03/30/Kafka深度解析-Kafka-Stream/","link":"","permalink":"https://www.wqp0010.top/2019/03/30/Kafka深度解析-Kafka-Stream/","excerpt":"","text":"Kafka Stream背景Kafka Stream是什么Kafka Stream是Apache Kafka从0.10版本引入的一个新Feature。它是提供了对存储于Kafka内的数据进行流式处理和分析的功能。Kafka Stream的特点如下： Kafka Stream提供了一个非常简单而轻量的Library，它可以非常方便地嵌入任意Java应用中，也可以任意方式打包和部署 除了Kafka外，无任何外部依赖 充分利用Kafka分区机制实现水平扩展和顺序性保证 通过可容错的state store实现高效的状态操作（如windowed join和aggregation） 支持正好一次处理语义 提供记录级的处理能力，从而实现毫秒级的低延迟 支持基于事件时间的窗口操作，并且可处理晚到的数据（late arrival of records） 同时提供底层的处理原语Processor（类似于Storm的spout和bolt），以及高层抽象的DSL（类似于Spark的map/group/reduce） 什么是流式计算一般流式计算会与批量计算相比较。在流式计算模型中，输入是持续的，可以认为在时间上是无界的，也就意味着，永远拿不到全量数据去做计算。同时，计算结果是持续输出的，也即计算结果在时间上也是无界的。流式计算一般对实时性要求较高，同时一般是先定义目标计算，然后数据到来之后将计算逻辑应用于数据。同时为了提高计算效率，往往尽可能采用增量计算代替全量计算。 stream_procissing 批量处理模型中，一般先有全量数据集，然后定义计算逻辑，并将计算应用于全量数据。特点是全量计算，并且计算结果一次性全量输出。 batch_procissing 为什么要有Kafka Stream当前已经有非常多的流式处理系统，最知名且应用最多的开源流式处理系统有Spark Streaming和Apache Storm。Apache Storm发展多年，应用广泛，提供记录级别的处理能力，当前也支持SQL on Stream。而Spark Streaming基于Apache Spark，可以非常方便与图计算，SQL处理等集成，功能强大，对于熟悉其它Spark应用开发的用户而言使用门槛低。另外，目前主流的Hadoop发行版，如MapR，Cloudera和Hortonworks，都集成了Apache Storm和Apache Spark，使得部署更容易。既然Apache Spark与Apache Storm拥用如此多的优势，那为何还需要Kafka Stream呢？笔者认为主要有如下原因。第一，Spark和Storm都是流式处理框架，而Kafka Stream提供的是一个基于Kafka的流式处理类库。框架要求开发者按照特定的方式去开发逻辑部分，供框架调用。开发者很难了解框架的具体运行方式，从而使得调试成本高，并且使用受限。而Kafka Stream作为流式处理类库，直接提供具体的类给开发者调用，整个应用的运行方式主要由开发者控制，方便使用和调试。 library 第二，虽然Cloudera与Hortonworks方便了Storm和Spark的部署，但是这些框架的部署仍然相对复杂。而Kafka Stream作为类库，可以非常方便的嵌入应用程序中，它对应用的打包和部署基本没有任何要求。更为重要的是，Kafka Stream充分利用了Kafka的分区机制和Consumer的Rebalance机制，使得Kafka Stream可以非常方便的水平扩展，并且各个实例可以使用不同的部署方式。具体来说，每个运行Kafka Stream的应用程序实例都包含了Kafka Consumer实例，多个同一应用的实例之间并行处理数据集。而不同实例之间的部署方式并不要求一致，比如部分实例可以运行在Web容器中，部分实例可运行在Docker或Kubernetes中。第三，就流式处理系统而言，基本都支持Kafka作为数据源。例如Storm具有专门的kafka-spout，而Spark也提供专门的spark-streaming-kafka模块。事实上，Kafka基本上是主流的流式处理系统的标准数据源。换言之，大部分流式系统中都已部署了Kafka，此时使用Kafka Stream的成本非常低。第四，使用Storm或Spark Streaming时，需要为框架本身的进程预留资源，如Storm的supervisor和Spark on YARN的node manager。即使对于应用实例而言，框架本身也会占用部分资源，如Spark Streaming需要为shuffle和storage预留内存。第五，由于Kafka本身提供数据持久化，因此Kafka Stream提供滚动部署和滚动升级以及重新计算的能力。第六，由于Kafka Consumer Rebalance机制，Kafka Stream可以在线动态调整并行度。 Kafka Stream架构Kafka Stream整体架构Kafka Stream的整体架构图如下所示。 Kafka_Stream_Architecture Kafka Stream的数据源只能如上图所示是Kafka。但是处理结果并不一定要如上图所示输出到Kafka。实际上KStream和Ktable的实例化都需要指定Topic。 KStream&lt;String, String&gt; stream = builder.stream(\"words-stream\");KTable&lt;String, String&gt; table = builder.table(\"words-table\", \"words-store\"); 另外，上图中的Consumer和Producer并不需要开发者在应用中显示实例化，而是由Kafka Stream根据参数隐式实例化和管理，从而降低了使用门槛。开发者只需要专注于开发核心业务逻辑，也即上图中Task内的部分。 Processor Topology基于Kafka Stream的流式应用的业务逻辑全部通过一个被称为Processor Topology的地方执行。它与Storm的Topology和Spark的DAG类似，都定义了数据在各个处理单元（在Kafka Stream中被称作Processor）间的流动方式，或者说定义了数据的处理逻辑。下面是一个Processor的示例，它实现了Word Count功能，并且每秒输出一次结果。public class WordCountProcessor implements Processor&lt;String, String&gt; &#123; private ProcessorContext context; private KeyValueStore&lt;String, Integer&gt; kvStore; @SuppressWarnings(\"unchecked\") @Override public void init(ProcessorContext context) &#123; this.context = context; this.context.schedule(1000); this.kvStore = (KeyValueStore&lt;String, Integer&gt;) context.getStateStore(\"Counts\"); &#125; @Override public void process(String key, String value) &#123; Stream.of(value.toLowerCase().split(\" \")).forEach((String word) -&gt; &#123; Optional&lt;Integer&gt; counts = Optional.ofNullable(kvStore.get(word)); int count = counts.map(wordcount -&gt; wordcount + 1).orElse(1); kvStore.put(word, count); &#125;); &#125; @Override public void punctuate(long timestamp) &#123; KeyValueIterator&lt;String, Integer&gt; iterator = this.kvStore.all(); iterator.forEachRemaining(entry -&gt; &#123; context.forward(entry.key, entry.value); this.kvStore.delete(entry.key); &#125;); context.commit(); &#125; @Override public void close() &#123; this.kvStore.close(); &#125;&#125; 从上述代码中可见 process定义了对每条记录的处理逻辑，也印证了Kafka可具有记录级的数据处理能力。 context.scheduler定义了punctuate被执行的周期，从而提供了实现窗口操作的能力。 context.getStateStore提供的状态存储为有状态计算（如窗口，聚合）提供了可能。 Kafka Stream并行模型Kafka Stream的并行模型中，最小粒度为Task，而每个Task包含一个特定子Topology的所有Processor。因此每个Task所执行的代码完全一样，唯一的不同在于所处理的数据集互补。这一点跟Storm的Topology完全不一样。Storm的Topology的每一个Task只包含一个Spout或Bolt的实例。因此Storm的一个Topology内的不同Task之间需要通过网络通信传递数据，而Kafka Stream的Task包含了完整的子Topology，所以Task之间不需要传递数据，也就不需要网络通信。这一点降低了系统复杂度，也提高了处理效率。如果某个Stream的输入Topic有多个(比如2个Topic，1个Partition数为4，另一个Partition数为3)，则总的Task数等于Partition数最多的那个Topic的Partition数（max(4,3)=4）。这是因为Kafka Stream使用了Consumer的Rebalance机制，每个Partition对应一个Task。下图展示了在一个进程（Instance）中以2个Topic（Partition数均为4）为数据源的Kafka Stream应用的并行模型。从图中可以看到，由于Kafka Stream应用的默认线程数为1，所以4个Task全部在一个线程中运行。 1_thread 为了充分利用多线程的优势，可以设置Kafka Stream的线程数。下图展示了线程数为2时的并行模型。 2_threads 前文有提到，Kafka Stream可被嵌入任意Java应用（理论上基于JVM的应用都可以）中，下图展示了在同一台机器的不同进程中同时启动同一Kafka Stream应用时的并行模型。注意，这里要保证两个进程的StreamsConfig.APPLICATION_ID_CONFIG完全一样。因为Kafka Stream将APPLICATION_ID_CONFIG作为隐式启动的Consumer的Group ID。只有保证APPLICATION_ID_CONFIG相同，才能保证这两个进程的Consumer属于同一个Group，从而可以通过Consumer Rebalance机制拿到互补的数据集。 2_instances 既然实现了多进程部署，可以以同样的方式实现多机器部署。该部署方式也要求所有进程的APPLICATION_ID_CONFIG完全一样。从图上也可以看到，每个实例中的线程数并不要求一样。但是无论如何部署，Task总数总会保证一致。 2_servers 这里对比一下Kafka Stream的Processor Topology与Storm的Topology。 Storm的Topology由Spout和Bolt组成，Spout提供数据源，而Bolt提供计算和数据导出。Kafka Stream的Processor Topology完全由Processor组成，因为它的数据固定由Kafka的Topic提供。 Storm的不同Bolt运行在不同的Executor中，很可能位于不同的机器，需要通过网络通信传输数据。而Kafka Stream的Processor Topology的不同Processor完全运行于同一个Task中，也就完全处于同一个线程，无需网络通信。 Storm的Topology可以同时包含Shuffle部分和非Shuffle部分，并且往往一个Topology就是一个完整的应用。而Kafka Stream的一个物理Topology只包含非Shuffle部分，而Shuffle部分需要通过through操作显示完成，该操作将一个大的Topology分成了2个子Topology。 Storm的Topology内，不同Bolt/Spout的并行度可以不一样，而Kafka Stream的子Topology内，所有Processor的并行度完全一样。 Storm的一个Task只包含一个Spout或者Bolt的实例，而Kafka Stream的一个Task包含了一个子Topology的所有Processor。 KTable vs. KStreamKTable和KStream是Kafka Stream中非常重要的两个概念，它们是Kafka实现各种语义的基础。因此这里有必要分析下二者的区别。KStream是一个数据流，可以认为所有记录都通过Insert only的方式插入进这个数据流里。而KTable代表一个完整的数据集，可以理解为数据库中的表。由于每条记录都是Key-Value对，这里可以将Key理解为数据库中的Primary Key，而Value可以理解为一行记录。可以认为KTable中的数据都是通过Update only的方式进入的。也就意味着，如果KTable对应的Topic中新进入的数据的Key已经存在，那么从KTable只会取出同一Key对应的最后一条数据，相当于新的数据更新了旧的数据。以下图为例，假设有一个KStream和KTable，基于同一个Topic创建，并且该Topic中包含如下图所示5条数据。此时遍历KStream将得到与Topic内数据完全一样的所有5条数据，且顺序不变。而此时遍历KTable时，因为这5条记录中有3个不同的Key，所以将得到3条记录，每个Key对应最新的值，并且这三条数据之间的顺序与原来在Topic中的顺序保持一致。这一点与Kafka的日志compact相同。 ktable_kstream 此时如果对该KStream和KTable分别基于key做Group，对Value进行Sum，得到的结果将会不同。对KStream的计算结果是&lt;Jack，4&gt;，&lt;Lily，7&gt;，&lt;Mike，4&gt;。而对Ktable的计算结果是&lt;Mike，4&gt;，&lt;Jack，3&gt;，&lt;Lily，5&gt;。 State store流式处理中，部分操作是无状态的，例如过滤操作（Kafka Stream DSL中用filer方法实现）。而部分操作是有状态的，需要记录中间状态，如Window操作和聚合计算。State store被用来存储中间状态。它可以是一个持久化的Key-Value存储，也可以是内存中的HashMap，或者是数据库。Kafka提供了基于Topic的状态存储。Topic中存储的数据记录本身是Key-Value形式的，同时Kafka的log compaction机制可对历史数据做compact操作，保留每个Key对应的最后一个Value，从而在保证Key不丢失的前提下，减少总数据量，从而提高查询效率。构造KTable时，需要指定其state store name。默认情况下，该名字也即用于存储该KTable的状态的Topic的名字，遍历KTable的过程，实际就是遍历它对应的state store，或者说遍历Topic的所有key，并取每个Key最新值的过程。为了使得该过程更加高效，默认情况下会对该Topic进行compact操作。另外，除了KTable，所有状态计算，都需要指定state store name，从而记录中间状态。 Kafka Stream如何解决流式系统中关键问题时间在流式数据处理中，时间是数据的一个非常重要的属性。从Kafka 0.10开始，每条记录除了Key和Value外，还增加了timestamp属性。目前Kafka Stream支持三种时间 事件发生时间。事件发生的时间，包含在数据记录中。发生时间由Producer在构造ProducerRecord时指定。并且需要Broker或者Topic将message.timestamp.type设置为CreateTime（默认值）才能生效。 消息接收时间，也即消息存入Broker的时间。当Broker或Topic将message.timestamp.type设置为LogAppendTime时生效。此时Broker会在接收到消息后，存入磁盘前，将其timestamp属性值设置为当前机器时间。一般消息接收时间比较接近于事件发生时间，部分场景下可代替事件发生时间。 消息处理时间，也即Kafka Stream处理消息时的时间。注：Kafka Stream允许通过实现org.apache.kafka.streams.processor.TimestampExtractor接口自定义记录时间 窗口前文提到，流式数据是在时间上无界的数据。而聚合操作只能作用在特定的数据集，也即有界的数据集上。因此需要通过某种方式从无界的数据集上按特定的语义选取出有界的数据。窗口是一种非常常用的设定计算边界的方式。不同的流式处理系统支持的窗口类似，但不尽相同。 Kafka Stream支持的窗口如下。 Hopping Time Window 该窗口定义如下图所示。它有两个属性，一个是Window size，一个是Advance interval。Window size指定了窗口的大小，也即每次计算的数据集的大小。而Advance interval定义输出的时间间隔。一个典型的应用场景是，每隔5秒钟输出一次过去1个小时内网站的PV或者UV。 Hopping_Time_Window Tumbling Time Window该窗口定义如下图所示。可以认为它是Hopping Time Window的一种特例，也即Window size和Advance interval相等。它的特点是各个Window之间完全不相交。 Sliding Window该窗口只用于2个KStream进行Join计算时。该窗口的大小定义了Join两侧KStream的数据记录被认为在同一个窗口的最大时间差。假设该窗口的大小为5秒，则参与Join的2个KStream中，记录时间差小于5的记录被认为在同一个窗口中，可以进行Join计算。 Session Window该窗口用于对Key做Group后的聚合操作中。它需要对Key做分组，然后对组内的数据根据业务需求定义一个窗口的起始点和结束点。一个典型的案例是，希望通过Session Window计算某个用户访问网站的时间。对于一个特定的用户（用Key表示）而言，当发生登录操作时，该用户（Key）的窗口即开始，当发生退出操作或者超时时，该用户（Key）的窗口即结束。窗口结束时，可计算该用户的访问时间或者点击次数等。 JoinKafka Stream由于包含KStream和Ktable两种数据集，因此提供如下Join计算 KTable Join KTable 结果仍为KTable。任意一边有更新，结果KTable都会更新。 KStream Join KStream 结果为KStream。必须带窗口操作，否则会造成Join操作一直不结束。 KStream Join KTable / GlobalKTable 结果为KStream。只有当KStream中有新数据时，才会触发Join计算并输出结果。KStream无新数据时，KTable的更新并不会触发Join计算，也不会输出数据。并且该更新只对下次Join生效。一个典型的使用场景是，KStream中的订单信息与KTable中的用户信息做关联计算。 对于Join操作，如果要得到正确的计算结果，需要保证参与Join的KTable或KStream中Key相同的数据被分配到同一个Task。具体方法是 参与Join的KTable或KStream的Key类型相同（实际上，业务含意也应该相同） 参与Join的KTable或KStream对应的Topic的Partition数相同Partitioner策略的最终结果等效（实现不需要完全一样，只要效果一样即可），也即Key相同的情况下，被分配到ID相同的Partition内 如果上述条件不满足，可通过调用如下方法使得它满足上述条件。 KStream&lt;K, V&gt; through(Serde&lt;K&gt; keySerde, Serde&lt;V&gt; valSerde, StreamPartitioner&lt;K, V&gt; partitioner, String topic) 聚合与乱序处理聚合操作可应用于KStream和KTable。当聚合发生在KStream上时必须指定窗口，从而限定计算的目标数据集。需要说明的是，聚合操作的结果肯定是KTable。因为KTable是可更新的，可以在晚到的数据到来时（也即发生数据乱序时）更新结果KTable。这里举例说明。假设对KStream以5秒为窗口大小，进行Tumbling Time Window上的Count操作。并且KStream先后出现时间为1秒, 3秒, 5秒的数据，此时5秒的窗口已达上限，Kafka Stream关闭该窗口，触发Count操作并将结果3输出到KTable中（假设该结果表示为&lt;1-5,3&gt;）。若1秒后，又收到了时间为2秒的记录，由于1-5秒的窗口已关闭，若直接抛弃该数据，则可认为之前的结果&lt;1-5,3&gt;不准确。而如果直接将完整的结果&lt;1-5,4&gt;输出到KStream中，则KStream中将会包含该窗口的2条记录，&lt;1-5,3&gt;, &lt;1-5,4&gt;，也会存在肮数据。因此Kafka Stream选择将聚合结果存于KTable中，此时新的结果&lt;1-5,4&gt;会替代旧的结果&lt;1-5,3&gt;。用户可得到完整的正确的结果。这种方式保证了数据准确性，同时也提高了容错性。但需要说明的是，Kafka Stream并不会对所有晚到的数据都重新计算并更新结果集，而是让用户设置一个retention period，将每个窗口的结果集在内存中保留一定时间，该窗口内的数据晚到时，直接合并计算，并更新结果KTable。超过retention period后，该窗口结果将从内存中删除，并且晚到的数据即使落入窗口，也会被直接丢弃。 容错Kafka Stream从如下几个方面进行容错 高可用的Partition保证无数据丢失。每个Task计算一个Partition，而Kafka数据复制机制保证了Partition内数据的高可用性，故无数据丢失风险。同时由于数据是持久化的，即使任务失败，依然可以重新计算。 状态存储实现快速故障恢复和从故障点继续处理。对于Join和聚合及窗口等有状态计算，状态存储可保存中间状态。即使发生Failover或Consumer Rebalance，仍然可以通过状态存储恢复中间状态，从而可以继续从Failover或Consumer Rebalance前的点继续计算。 KTable与retention period提供了对乱序数据的处理能力。 总结 Kafka Stream的并行模型完全基于Kafka的分区机制和Rebalance机制，实现了在线动态调整并行度 同一Task包含了一个子Topology的所有Processor，使得所有处理逻辑都在同一线程内完成，避免了不必的网络通信开销，从而提高了效率。 through方法提供了类似Spark的Shuffle机制，为使用不同分区策略的数据提供了Join的可能log compact提高了基于Kafka的state store的加载效率 state store为状态计算提供了可能 基于offset的计算进度管理以及基于state store的中间状态管理为发生Consumer rebalance或Failover时从断点处继续处理提供了可能，并为系统容错性提供了保障 KTable的引入，使得聚合计算拥用了处理乱序问题的能力","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://www.wqp0010.top/tags/Kafka/"},{"name":"大数据","slug":"大数据","permalink":"https://www.wqp0010.top/tags/大数据/"}]},{"title":"Kafka深度解析","slug":"Kafka深度解析","date":"2019-03-15T04:03:13.000Z","updated":"2020-06-30T06:30:12.913Z","comments":true,"path":"2019/03/15/Kafka深度解析/","link":"","permalink":"https://www.wqp0010.top/2019/03/15/Kafka深度解析/","excerpt":"","text":"背景介绍Kafka简介 Kafka是一种分布式的，基于发布/订阅的消息系统。主要设计目标如下： 以时间复杂度为O(1)的方式提供消息持久化能力，即使对TB级以上数据也能保证常数时间的访问性能 高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒100K条消息的传输 支持Kafka Server间的消息分区，及分布式消费，同时保证每个partition内的消息顺序传输 同时支持离线数据处理和实时数据处理 为什么要用消息系统 解耦 在项目启动之初来预测将来项目会碰到什么需求，是极其困难的。消息队列在处理过程中间插入了一个隐含的、基于数据的接口层，两边的处理过程都要实现这一接口。这允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束 冗余有些情况下，处理数据的过程会失败。除非数据被持久化，否则将造成丢失。消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。在被许多消息队列所采用的”插入-获取-删除”范式中，在把一个消息从队列中删除之前，需要你的处理过程明确的指出该消息已经被处理完毕，确保你的数据被安全的保存直到你使用完毕。 扩展性因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的；只要另外增加处理过程即可。不需要改变代码、不需要调节参数。扩展就像调大电力按钮一样简单。 灵活性 &amp; 峰值处理能力在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见；如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。 可恢复性当体系的一部分组件失效，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。而这种允许重试或者延后处理请求的能力通常是造就一个略感不便的用户和一个沮丧透顶的用户之间的区别。 送达保证消息队列提供的冗余机制保证了消息能被实际的处理，只要一个进程读取了该队列即可。在此基础上，部分消息系统提供了一个”只送达一次”保证。无论有多少进程在从队列中领取数据，每一个消息只能被处理一次。这之所以成为可能，是因为获取一个消息只是”预定”了这个消息，暂时把它移出了队列。除非客户端明确的表示已经处理完了这个消息，否则这个消息会被放回队列中去，在一段可配置的时间之后可再次被处理。 顺序保证在大多使用场景下，数据处理的顺序都很重要。消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。部分消息系统保证消息通过FIFO（先进先出）的顺序来处理，因此消息在队列中的位置就是从队列中检索他们的位置。 缓冲在任何重要的系统中，都会有需要不同的处理时间的元素。例如,加载一张图片比应用过滤器花费更少的时间。消息队列通过一个缓冲层来帮助任务最高效率的执行–写入队列的处理会尽可能的快速，而不受从队列读的预备处理的约束。该缓冲有助于控制和优化数据流经过系统的速度。 理解数据流在一个分布式系统里，要得到一个关于用户操作会用多长时间及其原因的总体印象，是个巨大的挑战。消息队列通过消息被处理的频率，来方便的辅助确定那些表现不佳的处理过程或领域，这些地方的数据流都不够优化。 异步通信很多时候，你不想也不需要立即处理消息。消息队列提供了异步处理机制，允许你把一个消息放入队列，但并不立即处理它。你想向队列中放入多少消息就放多少，然后在你乐意的时候再去处理它们。 常用Message Queue对比 RabbitMQ RabbitMQ是使用Erlang编写的一个开源的消息队列，本身支持很多的协议：AMQP，XMPP, SMTP, STOMP，也正因如此，它非常重量级，更适合于企业级的开发。同时实现了Broker构架，这意味着消息在发送给客户端时先在中心队列排队。对路由，负载均衡或者数据持久化都有很好的支持。 Redis Redis是一个基于Key-Value对的NoSQL数据库，开发维护很活跃。虽然它是一个Key-Value数据库存储系统，但它本身支持MQ功能，所以完全可以当做一个轻量级的队列服务来使用。对于RabbitMQ和Redis的入队和出队操作，各执行100万次，每10万次记录一次执行时间。测试数据分为128Bytes、512Bytes、1K和10K四个不同大小的数据。实验表明：入队时，当数据比较小时Redis的性能要高于RabbitMQ，而如果数据大小超过了10K，Redis则慢的无法忍受；出队时，无论数据大小，Redis都表现出非常好的性能，而RabbitMQ的出队性能则远低于Redis。 ZeroMQ ZeroMQ号称最快的消息队列系统，尤其针对大吞吐量的需求场景。ZMQ能够实现RabbitMQ不擅长的高级/复杂的队列，但是开发人员需要自己组合多种技术框架，技术上的复杂度是对这MQ能够应用成功的挑战。ZeroMQ具有一个独特的非中间件的模式，你不需要安装和运行一个消息服务器或中间件，因为你的应用程序将扮演了这个服务角色。你只需要简单的引用ZeroMQ程序库，可以使用NuGet安装，然后你就可以愉快的在应用程序之间发送消息了。但是ZeroMQ仅提供非持久性的队列，也就是说如果宕机，数据将会丢失。其中，Twitter的Storm 0.9.0以前的版本中默认使用ZeroMQ作为数据流的传输（Storm从0.9版本开始同时支持ZeroMQ和Netty作为传输模块）。 ActiveMQ ActiveMQ是Apache下的一个子项目。 类似于ZeroMQ，它能够以代理人和点对点的技术实现队列。同时类似于RabbitMQ，它少量代码就可以高效地实现高级应用场景。 Kafka/Jafka Kafka是Apache下的一个子项目，是一个高性能跨语言分布式发布/订阅消息队列系统，而Jafka是在Kafka之上孵化而来的，即Kafka的一个升级版。具有以下特性：快速持久化，可以在O(1)的系统开销下进行消息持久化；高吞吐，在一台普通的服务器上既可以达到10W/s的吞吐速率；完全的分布式系统，Broker、Producer、Consumer都原生自动支持分布式，自动实现负载均衡；支持Hadoop数据并行加载，对于像Hadoop的一样的日志数据和离线分析系统，但又要求实时处理的限制，这是一个可行的解决方案。Kafka通过Hadoop的并行加载机制来统一了在线和离线的消息处理。Apache Kafka相对于ActiveMQ是一个非常轻量级的消息系统，除了性能非常好之外，还是一个工作良好的分布式系统。 Kafka解析Terminology Broker Kafka集群包含一个或多个服务器，这种服务器被称为broker Topic 每条发布到Kafka集群的消息都有一个类别，这个类别被称为topic。（物理上不同topic的消息分开存储，逻辑上一个topic的消息虽然保存于一个或多个broker上但用户只需指定消息的topic即可生产或消费数据而不必关心数据存于何处） Partition parition是物理上的概念，每个topic包含一个或多个partition，创建topic时可指定parition数量。每个partition对应于一个文件夹，该文件夹下存储该partition的数据和索引文件 Producer 负责发布消息到Kafka broker Consumer 消费消息。每个consumer属于一个特定的consumer group（可为每个consumer指定group name，若不指定group name则属于默认的group）。使用consumer high level API时，同一topic的一条消息只能被同一个consumer group内的一个consumer消费，但多个consumer group可同时消费这一消息。 Kafka架构 KafkaArchitecture 如上图所示，一个典型的kafka集群中包含若干producer（可以是web前端产生的page view，或者是服务器日志，系统CPU、memory等），若干broker（Kafka支持水平扩展，一般broker数量越多，集群吞吐率越高），若干consumer group，以及一个Zookeeper集群。Kafka通过Zookeeper管理集群配置，选举leader，以及在consumer group发生变化时进行rebalance。producer使用push模式将消息发布到broker，consumer使用pull模式从broker订阅并消费消息。 Push vs. Pull 作为一个messaging system，Kafka遵循了传统的方式，选择由producer向broker push消息并由consumer从broker pull消息。一些logging-centric system，比如Facebook的Scribe和Cloudera的Flume,采用非常不同的push模式。事实上，push模式和pull模式各有优劣。 push模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。push模式的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据consumer的消费能力以适当的速率消费消息。 Topic &amp; Partition Topic在逻辑上可以被认为是一个queue。每条消费都必须指定它的topic，可以简单理解为必须指明把这条消息放进哪个queue里。为了使得Kafka的吞吐率可以水平扩展，物理上把topic分成一个或多个partition，每个partition在物理上对应一个文件夹，该文件夹下存储这个partition的所有消息和索引文件。 topic-partition 每个日志文件都是“log entries”序列，每一个log entry包含一个4字节整型数（值为N），其后跟N个字节的消息体。每条消息都有一个当前partition下唯一的64字节的offset，它指明了这条消息的起始位置。磁盘上存储的消息格式如下： message length ： 4 bytes (value: 1+4+n) “magic” value ： 1 byte crc ： 4 bytes payload ： n bytes这个“log entries”并非由一个文件构成，而是分成多个segment，每个segment名为该segment第一条消息的offset和“.kafka”组成。另外会有一个索引文件，它标明了每个segment下包含的log entry的offset范围，如下图所示。 partition_segment 因为每条消息都被append到该partition中，是顺序写磁盘，因此效率非常高（经验证，顺序写磁盘效率比随机写内存还要高，这是Kafka高吞吐率的一个很重要的保证）。 partition 每一条消息被发送到broker时，会根据paritition规则选择被存储到哪一个partition。如果partition规则设置的合理，所有消息可以均匀分布到不同的partition里，这样就实现了水平扩展。（如果一个topic对应一个文件，那这个文件所在的机器I/O将会成为这个topic的性能瓶颈，而partition解决了这个问题）。在创建topic时可以在$KAFKA_HOME/config/server.properties中指定这个partition的数量(如下所示)，当然也可以在topic创建之后去修改parition数量。 # The default number of log partitions per topic. More partitions allow greater# parallelism for consumption, but this will also result in more files across# the brokers.num.partitions=3 在发送一条消息时，可以指定这条消息的key，producer根据这个key和partition机制来判断将这条消息发送到哪个parition。paritition机制可以通过指定producer的paritition. class这一参数来指定，该class必须实现kafka.producer.Partitioner接口。本例中如果key可以被解析为整数则将对应的整数与partition总数取余，该消息会被发送到该数对应的partition。（每个parition都会有个序号） import kafka.producer.Partitioner;import kafka.utils.VerifiableProperties;public class MyPartitioner&lt;T&gt; implements Partitioner &#123; public MyPartitioner(VerifiableProperties verifiableProperties) &#123;&#125; @Override public int partition(Object key, int numPartitions) &#123; try &#123; int partitionNum = Integer.parseInt((String) key); return Math.abs(Integer.parseInt((String) key) % numPartitions); &#125; catch (Exception e) &#123; return Math.abs(key.hashCode() % numPartitions); &#125; &#125;&#125; 如果将上例中的class作为partitioner.class，并通过如下代码发送20条消息（key分别为0，1，2，3）至topic2（包含4个partition）。 public void sendMessage() throws InterruptedException&#123; for(int i = 1; i &lt;= 5; i++)&#123; List messageList = new ArrayList&lt;KeyedMessage&lt;String, String&gt;&gt;(); for(int j = 0; j &lt; 4; j++）&#123; messageList.add(new KeyedMessage&lt;String, String&gt;(\"topic2\", j+\"\", \"The \" + i + \" message for key \" + j)); &#125; producer.send(messageList); &#125; producer.close();&#125; 则key相同的消息会被发送并存储到同一个partition里，而且key的序号正好和partition序号相同。（partition序号从0开始，本例中的key也正好从0开始）。如下图所示。 partition_key 对于传统的message queue而言，一般会删除已经被消费的消息，而Kafka集群会保留所有的消息，无论其被消费与否。当然，因为磁盘限制，不可能永久保留所有数据（实际上也没必要），因此Kafka提供两种策略去删除旧数据。一是基于时间，二是基于partition文件大小。例如可以通过配置$KAFKA_HOME/config/server.properties，让Kafka删除一周前的数据，也可通过配置让Kafka在partition文件超过1GB时删除旧数据，如下所示。 ############################# Log Retention Policy ############################## The following configurations control the disposal of log segments. The policy can# be set to delete segments after a period of time, or after a given size has accumulated.# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens# from the end of the log.# The minimum age of a log file to be eligible for deletionlog.retention.hours=168# A size-based retention policy for logs. Segments are pruned from the log as long as the remaining# segments do not drop below log.retention.bytes.#log.retention.bytes=1073741824# The maximum size of a log segment file. When this size is reached a new log segment will be created.log.segment.bytes=1073741824# The interval at which log segments are checked to see if they can be deleted according# to the retention policieslog.retention.check.interval.ms=300000# By default the log cleaner is disabled and the log retention policy will default to#just delete segments after their retention expires.# If log.cleaner.enable=true is set the cleaner will be enabled and individual logs#can then be marked for log compaction.log.cleaner.enable=false 这里要注意，因为Kafka读取特定消息的时间复杂度为O(1)，即与文件大小无关，所以这里删除文件与Kafka性能无关，选择怎样的删除策略只与磁盘以及具体的需求有关。另外，Kafka会为每一个consumer group保留一些metadata信息–当前消费的消息的position，也即offset。这个offset由consumer控制。正常情况下consumer会在消费完一条消息后线性增加这个offset。当然，consumer也可将offset设成一个较小的值，重新消费一些消息。因为offet由consumer控制，所以Kafka broker是无状态的，它不需要标记哪些消息被哪些consumer过，不需要通过broker去保证同一个consumer group只有一个consumer能消费某一条消息，因此也就不需要锁机制，这也为Kafka的高吞吐率提供了有力保障。 Replication &amp; Leader election Kafka从0.8开始提供partition级别的replication，replication的数量可在$KAFKA_HOME/config/server.properties中配置。 default.replication.factor = 1 该 Replication与leader election配合提供了自动的failover机制。replication对Kafka的吞吐率是有一定影响的，但极大的增强了可用性。默认情况下，Kafka的replication数量为1。 每个partition都有一个唯一的leader，所有的读写操作都在leader上完成，leader批量从leader上pull数据。一般情况下partition的数量大于等于broker的数量，并且所有partition的leader均匀分布在broker上。follower上的日志和其leader上的完全一样。 和大部分分布式系统一样，Kakfa处理失败需要明确定义一个broker是否alive。对于Kafka而言，Kafka存活包含两个条件，一是它必须维护与Zookeeper的session(这个通过Zookeeper的heartbeat机制来实现)。二是follower必须能够及时将leader的writing复制过来，不能“落后太多”。 leader会track“in sync”的node list。如果一个follower宕机，或者落后太多，leader将把它从”in sync” list中移除。这里所描述的“落后太多”指follower复制的消息落后于leader后的条数超过预定值，该值可在$KAFKA_HOME/config/server.properties中配置 #If a replica falls more than this many messages behind the leader, the leader will remove the follower from ISR and treat it as deadreplica.lag.max.messages=4000#If a follower has not sent any fetch requests for this window of time, the leader will remove the follower from ISR (in-sync replicas) and treat it as deadreplica.lag.time.max.ms=10000 需要说明的是，Kafka只解决”fail/recover”，不处理“Byzantine”（“拜占庭”）问题。 一条消息只有被“in sync” list里的所有follower都从leader复制过去才会被认为已提交。这样就避免了部分数据被写进了leader，还没来得及被任何follower复制就宕机了，而造成数据丢失（consumer无法消费这些数据）。而对于producer而言，它可以选择是否等待消息commit，这可以通过request.required.acks来设置。这种机制确保了只要“in sync” list有一个或以上的flollower，一条被commit的消息就不会丢失。 这里的复制机制即不是同步复制，也不是单纯的异步复制。事实上，同步复制要求“活着的”follower都复制完，这条消息才会被认为commit，这种复制方式极大的影响了吞吐率（高吞吐率是Kafka非常重要的一个特性）。而异步复制方式下，follower异步的从leader复制数据，数据只要被leader写入log就被认为已经commit，这种情况下如果follwer都落后于leader，而leader突然宕机，则会丢失数据。而Kafka的这种使用“in sync” list的方式则很好的均衡了确保数据不丢失以及吞吐率。follower可以批量的从leader复制数据，这样极大的提高复制性能（批量写磁盘），极大减少了follower与leader的差距（前文有说到，只要follower落后leader不太远，则被认为在“in sync” list里）。 上文说明了Kafka是如何做replication的，另外一个很重要的问题是当leader宕机了，怎样在follower中选举出新的leader。因为follower可能落后许多或者crash了，所以必须确保选择“最新”的follower作为新的leader。一个基本的原则就是，如果leader不在了，新的leader必须拥有原来的leader commit的所有消息。这就需要作一个折衷，如果leader在标明一条消息被commit前等待更多的follower确认，那在它die之后就有更多的follower可以作为新的leader，但这也会造成吞吐率的下降。 一种非常常用的选举leader的方式是“majority vote”（“少数服从多数”），但Kafka并未采用这种方式。这种模式下，如果我们有2f+1个replica（包含leader和follower），那在commit之前必须保证有f+1个replica复制完消息，为了保证正确选出新的leader，fail的replica不能超过f个。因为在剩下的任意f+1个replica里，至少有一个replica包含有最新的所有消息。这种方式有个很大的优势，系统的latency只取决于最快的几台server，也就是说，如果replication factor是3，那latency就取决于最快的那个follower而非最慢那个。majority vote也有一些劣势，为了保证leader election的正常进行，它所能容忍的fail的follower个数比较少。如果要容忍1个follower挂掉，必须要有3个以上的replica，如果要容忍2个follower挂掉，必须要有5个以上的replica。也就是说，在生产环境下为了保证较高的容错程度，必须要有大量的replica，而大量的replica又会在大数据量下导致性能的急剧下降。这就是这种算法更多用在Zookeeper这种共享集群配置的系统中而很少在需要存储大量数据的系统中使用的原因。例如HDFS的HA feature是基于majority-vote-based journal，但是它的数据存储并没有使用这种expensive的方式。 实际上，leader election算法非常多，比如Zookeper的Zab, Raft和Viewstamped Replication。而Kafka所使用的leader election算法更像微软的PacificA算法。 Kafka在Zookeeper中动态维护了一个ISR（in-sync replicas） set，这个set里的所有replica都跟上了leader，只有ISR里的成员才有被选为leader的可能。在这种模式下，对于f+1个replica，一个Kafka topic能在保证不丢失已经ommit的消息的前提下容忍f个replica的失败。在大多数使用场景中，这种模式是非常有利的。事实上，为了容忍f个replica的失败，majority vote和ISR在commit前需要等待的replica数量是一样的，但是ISR需要的总的replica的个数几乎是majority vote的一半。 虽然majority vote与ISR相比有不需等待最慢的server这一优势，但是Kafka作者认为Kafka可以通过producer选择是否被commit阻塞来改善这一问题，并且节省下来的replica和磁盘使得ISR模式仍然值得。 上文提到，在ISR中至少有一个follower时，Kafka可以确保已经commit的数据不丢失，但如果某一个partition的所有replica都挂了，就无法保证数据不丢失了。这种情况下有两种可行的方案： 等待ISR中的任一个replica“活”过来，并且选它作为leader 选择第一个“活”过来的replica（不一定是ISR中的）作为leader 这就需要在可用性和一致性当中作出一个简单的平衡。如果一定要等待ISR中的replica“活”过来，那不可用的时间就可能会相对较长。而且如果ISR中的所有replica都无法“活”过来了，或者数据都丢失了，这个partition将永远不可用。选择第一个“活”过来的replica作为leader，而这个replica不是ISR中的replica，那即使它并不保证已经包含了所有已commit的消息，它也会成为leader而作为consumer的数据源（前文有说明，所有读写都由leader完成）。Kafka0.8.*使用了第二种方式。根据Kafka的文档，在以后的版本中，Kafka支持用户通过配置选择这两种方式中的一种，从而根据不同的使用场景选择高可用性还是强一致性。 上文说明了一个parition的replication过程，然尔Kafka集群需要管理成百上千个partition，Kafka通过round-robin的方式来平衡partition从而避免大量partition集中在了少数几个节点上。同时Kafka也需要平衡leader的分布，尽可能的让所有partition的leader均匀分布在不同broker上。另一方面，优化leadership election的过程也是很重要的，毕竟这段时间相应的partition处于不可用状态。一种简单的实现是暂停宕机的broker上的所有partition，并为之选举leader。实际上，Kafka选举一个broker作为controller，这个controller通过watch Zookeeper检测所有的broker failure，并负责为所有受影响的parition选举leader，再将相应的leader调整命令发送至受影响的broker，过程如下图所示。 controller 这样做的好处是，可以批量的通知leadership的变化，从而使得选举过程成本更低，尤其对大量的partition而言。如果controller失败了，幸存的所有broker都会尝试在Zookeeper中创建/controller-&gt;{this broker id}，如果创建成功（只可能有一个创建成功），则该broker会成为controller，若创建不成功，则该broker会等待新controller的命令。 controller_failover Consumer group （本节所有描述都是基于consumer hight level API而非low level API）。 每一个consumer实例都属于一个consumer group，每一条消息只会被同一个consumer group里的一个consumer实例消费。（不同consumer group可以同时消费同一条消息） consumer_group 很多传统的message queue都会在消息被消费完后将消息删除，一方面避免重复消费，另一方面可以保证queue的长度比较少，提高效率。而如上文所将，Kafka并不删除已消费的消息，为了实现传统message queue消息只被消费一次的语义，Kafka保证保证同一个consumer group里只有一个consumer会消费一条消息。与传统message queue不同的是，Kafka还允许不同consumer group同时消费同一条消息，这一特性可以为消息的多元化处理提供了支持。实际上，Kafka的设计理念之一就是同时提供离线处理和实时处理。根据这一特性，可以使用Storm这种实时流处理系统对消息进行实时在线处理，同时使用Hadoop这种批处理系统进行离线处理，还可以同时将数据实时备份到另一个数据中心，只需要保证这三个操作所使用的consumer在不同的consumer group即可。下图展示了Kafka在Linkedin的一种简化部署。 kafka_in_linkedin 为了更清晰展示Kafka consumer group的特性，笔者作了一项测试。创建一个topic (名为topic1)，创建一个属于group1的consumer实例，并创建三个属于group2的consumer实例，然后通过producer向topic1发送key分别为1，2，3r的消息。结果发现属于group1的consumer收到了所有的这三条消息，同时group2中的3个consumer分别收到了key为1，2，3的消息。如下图所示。 consumer_group_test Consumer Rebalance （本节所讲述内容均基于Kafka consumer high level API） Kafka保证同一consumer group中只有一个consumer会消费某条消息，实际上，Kafka保证的是稳定状态下每一个consumer实例只会消费某一个或多个特定partition的数据，而某个partition的数据只会被某一个特定的consumer实例所消费。这样设计的劣势是无法让同一个consumer group里的consumer均匀消费数据，优势是每个consumer不用都跟大量的broker通信，减少通信开销，同时也降低了分配难度，实现也更简单。另外，因为同一个partition里的数据是有序的，这种设计可以保证每个partition里的数据也是有序被消费。 如果某consumer group中consumer数量少于partition数量，则至少有一个consumer会消费多个partition的数据，如果consumer的数量与partition数量相同，则正好一个consumer消费一个partition的数据，而如果consumer的数量多于partition的数量时，会有部分consumer无法消费该topic下任何一条消息。 如下例所示，如果topic1有0，1，2共三个partition，当group1只有一个consumer(名为consumer1)时，该 consumer可消费这3个partition的所有数据。 group1_consumer1 增加一个consumer(consumer2)后，其中一个consumer（consumer1）可消费2个partition的数据，另外一个consumer(consumer2)可消费另外一个partition的数据。 group1_consumer_1_2 再增加一个consumer(consumer3)后，每个consumer可消费一个partition的数据。consumer1消费partition0，consumer2消费partition1，consumer3消费partition2 group1_consumer_1_2_3 再增加一个consumer（consumer4）后，其中3个consumer可分别消费一个partition的数据，另外一个consumer（consumer4）不能消费topic1任何数据。 group1_consumer_1_2_3_4 此时关闭consumer1，剩下的consumer可分别消费一个partition的数据。 group1_consumer_2_3_4 接着关闭consumer2，剩下的consumer3可消费2个partition，consumer4可消费1个partition。 group1_consumer_3_4 再关闭consumer3，剩下的consumer4可同时消费topic1的3个partition。 group1_consumer_4.png consumer rebalance算法如下： Sort PT (all partitions in topic T) Sort CG(all consumers in consumer group G) Let i be the index position of Ci in CG and let N=size(PT)/size(CG) Remove current entries owned by Ci from the partition owner registry Assign partitions from iN to (i+1)N-1 to consumer Ci Add newly assigned partitions to the partition owner registry 目前consumer rebalance的控制策略是由每一个consumer通过Zookeeper完成的。具体的控制方式如下： Register itself in the consumer id registry under its group. Register a watch on changes under the consumer id registry. Register a watch on changes under the broker id registry. If the consumer creates a message stream using a topic filter, it also registers a watch on changes under the broker topic registry. Force itself to rebalance within in its consumer group. 在这种策略下，每一个consumer或者broker的增加或者减少都会触发consumer rebalance。因为每个consumer只负责调整自己所消费的partition，为了保证整个consumer group的一致性，所以当一个consumer触发了rebalance时，该consumer group内的其它所有consumer也应该同时触发rebalance。 ######消息Deliver guarantee 通过上文介绍，想必读者已经明天了producer和consumer是如何工作的，以及Kafka是如何做replication的，接下来要讨论的是Kafka如何确保消息在producer和consumer之间传输。有这么几种可能的delivery guarantee： At most once 消息可能会丢，但绝不会重复传输 At least one 消息绝不会丢，但可能会重复传输 Exactly once 每条消息肯定会被传输一次且仅传输一次，很多时候这是用户所想要的。 Kafka的delivery guarantee semantic非常直接。当producer向broker发送消息时，一旦这条消息被commit，因数replication的存在，它就不会丢。但是如果producer发送数据给broker后，遇到的网络问题而造成通信中断，那producer就无法判断该条消息是否已经commit。这一点有点像向一个自动生成primary key的数据库表中插入数据。虽然Kafka无法确定网络故障期间发生了什么，但是producer可以生成一种类似于primary key的东西，发生故障时幂等性的retry多次，这样就做到了Exactly one。 接下来讨论的是消息从broker到consumer的delivery guarantee semantic。（仅针对Kafka consumer high level API）。consumer在从broker读取消息后，可以选择commit，该操作会在Zookeeper中存下该consumer在该partition下读取的消息的offset。该consumer下一次再读该partition时会从下一条开始读取。如未commit，下一次读取的开始位置会跟上一次commit之后的开始位置相同。当然可以将consumer设置为autocommit，即consumer一旦读到数据立即自动commit。如果只讨论这一读取消息的过程，那Kafka是确保了Exactly once。但实际上实际使用中consumer并非读取完数据就结束了，而是要进行进一步处理，而数据处理与commit的顺序在很大程度上决定了消息从broker和consumer的delivery guarantee semantic。读完消息先commit再处理消息。这种模式下，如果consumer在commit后还没来得及处理消息就crash了，下次重新开始工作后就无法读到刚刚已提交而未处理的消息，这就对应于At most once读完消息先处理再commit。这种模式下，如果处理完了消息在commit之前consumer crash了，下次重新开始工作时还会处理刚刚未commit的消息，实际上该消息已经被处理过了。这就对应于At least once。在很多情况使用场景下，消息都有一个primary key，所以消息的处理往往具有幂等性，即多次处理这一条消息跟只处理一次是等效的，那就可以认为是Exactly once。（人个感觉这种说法有些牵强，毕竟它不是Kafka本身提供的机制，而且primary key本身不保证操作的幂等性。而且实际上我们说delivery guarantee semantic是讨论被处理多少次，而非处理结果怎样，因为处理方式多种多样，我们的系统不应该把处理过程的特性–如是否幂等性，当成Kafka本身的feature）如果一定要做到Exactly once，就需要协调offset和实际操作的输出。精典的做法是引入两阶段提交。如果能让offset和操作输入存在同一个地方，会更简洁和通用。这种方式可能更好，因为许多输出系统可能不支持两阶段提交。比如，consumer拿到数据后可能把数据放到HDFS，如果把最新的offset和数据本身一起写到HDFS，那就可以保证数据的输出和offset的更新要么都完成，要么都不完成，间接实现Exactly once。（目前就high level API而言，offset是存于Zookeeper中的，无法存于HDFS，而low level API的offset是由自己去维护的，可以将之存于HDFS中） 总之，Kafka默认保证At least once，并且允许通过设置producer异步提交来实现At most once。而Exactly once要求与目标存储系统协作，幸运的是Kafka提供的offset可以使用这种方式非常直接非常容易。","categories":[],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://www.wqp0010.top/tags/Kafka/"},{"name":"大数据","slug":"大数据","permalink":"https://www.wqp0010.top/tags/大数据/"}]},{"title":"SpringBoot2.x实现redis共享session","slug":"SpringBoot2-x实现redis共享session","date":"2019-02-27T15:17:43.000Z","updated":"2020-06-30T00:33:58.866Z","comments":true,"path":"2019/02/27/SpringBoot2-x实现redis共享session/","link":"","permalink":"https://www.wqp0010.top/2019/02/27/SpringBoot2-x实现redis共享session/","excerpt":"","text":"前言本篇说一下SpringBoot使用redis共享session的方法 导入依赖&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.session&lt;/groupId&gt; &lt;artifactId&gt;spring-session-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 配置redisspring: application: name: session-redis redis: database: 0 host: \"localhost\" port: 6379 password: \"\" jedis: pool: max-active: 8 max-idle: 0 max-wait: -1ms timeout: 100ms session: #session超时时间 timeout: 1800 namespace: \"session-redis\" 配置类@Configurationpublic class SessionConfig extends RedisHttpSessionConfiguration &#123; @Value(\"$&#123;spring.session.timeout&#125;\") private Integer sessionTimeoutInSec; @Value(\"$&#123;spring.session.namespace&#125;\") private String sessionRedisNamespace; @Bean public LettuceConnectionFactory connectionFactory() &#123; return new LettuceConnectionFactory(); &#125; @PostConstruct public void initConfig() throws Exception &#123; this.setMaxInactiveIntervalInSeconds(sessionTimeoutInSec); this.setRedisNamespace(sessionRedisNamespace); &#125;&#125; 测试写一个controller： @RestControllerpublic class IndexController &#123; private final static DateTimeFormatter formatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\"); @RequestMapping(name = \"/login\", method = RequestMethod.POST) public String login(HttpSession session, String username, String pwd) &#123; if (StringUtils.isBlank(username) || StringUtils.isBlank(pwd)) &#123; return \"username和pwd不能为空\"; &#125; session.setAttribute(\"username\", username); session.setAttribute(\"logintime\",LocalDateTime.now()); return username + \"登陆成功\"; &#125; @RequestMapping(name = \"/\", method = RequestMethod.GET) public String index(HttpSession session) &#123; String username = (String) session.getAttribute(\"username\"); if (StringUtils.isBlank(username)) &#123; return \"您还未登录！\"; &#125; else &#123; return \"欢迎您\" + username + \",登陆时间为\" + formatter.format((LocalDateTime)session.getAttribute(\"logintime\")); &#125; &#125;&#125; 首先访问localhost:8080/login登陆，即可在redis看到共享的session信息 至此，实现完成 参考 spring-session httpsession-redis","categories":[],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://www.wqp0010.top/tags/SpringBoot/"},{"name":"session","slug":"session","permalink":"https://www.wqp0010.top/tags/session/"},{"name":"redis","slug":"redis","permalink":"https://www.wqp0010.top/tags/redis/"}]},{"title":"SpringBoot2.x实现mysql共享session","slug":"SpringBoot2-x实现mysql共享session","date":"2019-02-15T15:00:03.000Z","updated":"2020-06-30T00:33:58.866Z","comments":true,"path":"2019/02/15/SpringBoot2-x实现mysql共享session/","link":"","permalink":"https://www.wqp0010.top/2019/02/15/SpringBoot2-x实现mysql共享session/","excerpt":"","text":"前言网上的例子大部分是使用redis解决session共享问题，其实spring还提供了其他方案：Spring Session Core - provides core Spring Session functionalities and APIsSpring Session Data Redis - provides SessionRepository and ReactiveSessionRepository implementation backed by Redis and configuration supportSpring Session JDBC - provides SessionRepository implementation backed by a relational database and configuration supportSpring Session Hazelcast - provides SessionRepository implementation backed by Hazelcast and configuration support接下来讲使用mysql的方式 创建数据库DROP TABLE IF EXISTS SPRING_SESSION_ATTRIBUTES;DROP TABLE IF EXISTS SPRING_SESSION;CREATE TABLE SPRING_SESSION ( PRIMARY_ID CHAR(36) NOT NULL, SESSION_ID CHAR(36) NOT NULL, CREATION_TIME BIGINT NOT NULL, LAST_ACCESS_TIME BIGINT NOT NULL, MAX_INACTIVE_INTERVAL INT NOT NULL, EXPIRY_TIME BIGINT NOT NULL, PRINCIPAL_NAME VARCHAR(100), CONSTRAINT SPRING_SESSION_PK PRIMARY KEY (PRIMARY_ID)) ENGINE=INNODB ROW_FORMAT=DYNAMIC;CREATE UNIQUE INDEX SPRING_SESSION_IX1 ON SPRING_SESSION (SESSION_ID);CREATE INDEX SPRING_SESSION_IX2 ON SPRING_SESSION (EXPIRY_TIME);CREATE INDEX SPRING_SESSION_IX3 ON SPRING_SESSION (PRINCIPAL_NAME);CREATE TABLE SPRING_SESSION_ATTRIBUTES ( SESSION_PRIMARY_ID CHAR(36) NOT NULL, ATTRIBUTE_NAME VARCHAR(200) NOT NULL, ATTRIBUTE_BYTES BLOB NOT NULL, CONSTRAINT SPRING_SESSION_ATTRIBUTES_PK PRIMARY KEY (SESSION_PRIMARY_ID, ATTRIBUTE_NAME), CONSTRAINT SPRING_SESSION_ATTRIBUTES_FK FOREIGN KEY (SESSION_PRIMARY_ID) REFERENCES SPRING_SESSION(PRIMARY_ID) ON DELETE CASCADE) ENGINE=INNODB ROW_FORMAT=DYNAMIC; 这里创建了两个表，一个保存session一个保存session绑定的属性。该表结构是spring-session-jdbc指定的，不要修改结构 引入依赖&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.session&lt;/groupId&gt; &lt;artifactId&gt;spring-session-jdbc&lt;/artifactId&gt; &lt;version&gt;2.1.3.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;&lt;/dependency&gt; 当引入了spring-session-jdbc后会自动包含spring-session-core 配置sessionspring: datasource: #连接信息 url: jdbc:mysql://localhost:3306/springboot?useUnicode=true&amp;characterEncoding=utf8&amp;serverTimezone=GMT%2B8&amp;useSSL=false username: root password: 123456 driver-class-name: com.mysql.cj.jdbc.Driver session: #session超时时间 timeout: 120s jdbc: #清理session计划任务，默认是每分钟一次 cleanup-cron: 0 * * * * * 当依赖中只有一个spring-session-jdbc时，不需要再指定spring.session.store-type=JDBC。只有当存在多个实现(spring-session-redis)时才需要指定 使用@RestControllerpublic class IndexController &#123; @RequestMapping(name = \"/login\", method = RequestMethod.POST) public String login(HttpSession session, String username, String pwd) &#123; if (StringUtils.isBlank(username) || StringUtils.isBlank(pwd)) &#123; return \"username和pwd不能为空\"; &#125; session.setAttribute(\"username\", username); session.setAttribute(\"logintime\",LocalDateTime.now()); return username + \"登陆成功\"; &#125; @RequestMapping(name = \"/\", method = RequestMethod.GET) public String index(HttpSession session) &#123; String username = (String) session.getAttribute(\"username\"); if (StringUtils.isBlank(username)) &#123; return \"您还未登录！\"; &#125; else &#123; return \"欢迎您\" + username + \",登陆时间为\" + session.getAttribute(\"logintime\"); &#125; &#125;&#125; 首先访问localhost:8080/login登陆，可以看到数据库中的记录SPRING_SESSION表 SPRING_SESSION SPRING_SESSION_ATTRIBUTES表 SPRING_SESSION_ATTRIBUTES 至此，实现完成 参考 spring-session httpsession-jdbc-boot","categories":[],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://www.wqp0010.top/tags/SpringBoot/"},{"name":"mysql","slug":"mysql","permalink":"https://www.wqp0010.top/tags/mysql/"},{"name":"session","slug":"session","permalink":"https://www.wqp0010.top/tags/session/"}]},{"title":"docker命令 (三)","slug":"docker命令-三","date":"2019-01-09T14:37:40.000Z","updated":"2020-06-30T00:33:58.866Z","comments":true,"path":"2019/01/09/docker命令-三/","link":"","permalink":"https://www.wqp0010.top/2019/01/09/docker命令-三/","excerpt":"","text":"基本概念 container 容器。可以把每个 container 看做是一个独立的主机。 container 的创建通常有一个 image 作为其模板。类比成虚拟机的话可以理解为 image 就是虚拟机的镜像，而 container 就是一个个正在运行的虚拟机。一个虚拟机镜像可以创建出多个运行的虚拟主机且相互独立。 注意：container 一旦创建如果没有用 rm 命令移除，将会一直存在。所以用完后记得删除哦。 image 镜像。image 相当于 container 的模板，container 创建后里面有什么软件完全取决于它使用什么 image 。image 可以通过 container 创建（相当于把此时 container 的状态保存成快照），也可以通过 Dockerfile （一个文本文件，里面使用 docker 规定的一些写法）来创建。其中通过 Dockerfile 创建的方法能让环境配置和代码一起被版本库一起管理。 registry 存放镜像的仓库。只要能连接到 registry 每个人都可以很方便地通过 pull 命令从仓库中获取镜像。docker 默认使用的仓库是 docker hub，国内可以使用 DaoCloud 来建立 Mirror 连接到 docker hub，进而加快获取 image 的速度。 boot2docker 一个轻量级 linux 虚拟机，主要是为了让非 linux 系统也能用上 docker 。它实质上是一个 virtualbox 虚拟主机+一个能管理这个虚拟主机的命令行工具。由于这个虚拟主机的存在，在非 linux 系统上 container 需要获取一些物理系统资源（如 usb 设备）时不仅需要配置 docker 命令，还需要配置 boot2docker 这个虚拟主机的资源配置。 常用命令表 container 相关 操作 命令 示例 创建 container docker create docker create nginx:latest 创建并运行 container docker run docker run nginx:latest /bin/bash 创建并运行 container 后进入其 bash 控制台 docker run -t -i image /bin/bash docker run -t -i ubuntu /bin/bash 创建并运行 container 并让其在后台运行，并端口映射 docker run -p [port in container]:[port in physical system] -d [image] [command] docker run -p 5000:5000 -d training/webapp python app.py 查看正在运行的所有 container 信息 docker ps docker ps 查看最后创建的 container docker ps -l docker ps -l 查看所有 container ，包括正在运行和已经关闭的 docker ps -a docker ps -a 输出指定 container 的 stdout 信息（用来看 log ，效果和 tail -f 类似，会实时输出。） docker logs -f [container] docker logs -f nostalgic_morse 获取 container 指定端口映射关系 docker port [container] [port] docker port nostalgic_morse 5000 查看 container 进程列表 docker top [container] docker top nostalgic_morse 查看 container 详细信息 docker inspect [container] docker inspect nostalgic_morse 停止 continer docker stop [container] docker stop nostalgic_morse 强制停止 container docker kill [container] docker kill nostalgic_morse 启动一个已经停止的 container docker start [container] docker start nostalgic_morse 重启 container (若 container 处于关闭状态，则直接启动) docker restart [container] docker restart nostalgic_morse 删除 container docker rm [container] docker rm nostalgic_morse image 相关 操作 命令 示例 从 container 创建 image docker commit [container] [imageName] docker commit nostalgic_morse ouruser/sinatra:v2 从 Dockerfile 创建 image docker build -t [imageName] [pathToFolder] docker build ouruser/sinatra:v3 . 查看本地所有 image docker images docker images 在 registry 中搜索镜像 docker search [query] docker search ubuntu 从 registry 中获取镜像 （若无指定 tag 名称，则默认使用 latest 这个 tag） docker pull [imageName] docker pull ubuntu:14.04, docker pull training/webapp 给 image 打 tag docker tag [imageId] [imageName] docker tag 5db5f8471261 ouruser/sinatra:devel 把本地 image 上传到 registry 中 (此时会把所有 tag 都上传上去) docker push [imageName] docker push ouruser/sinatra 删除本地 image docker rmi [image] docker rmi training/sinatra 总结docker 虽然是一个虚拟化技术，但使用上却更像是在管理系统软件或者代码。里面的一些 ps，top，rm 命令让使用 Linux 命令的人感到十分亲切（虽然它们的语义有点不一样。。。），start，stop，restart 让你感觉像是在控制 service ，而 push，pull，commit，tag 又让你觉得像是在使用 git 。因此程序员会感到很亲切且容易上手。 同时由于可以使用 Dockerfile 进行 image 的构建，且 docker hub 支持从 github 等地方自动根据 Dockerfile 进行构建，所以 docker 把运行环境也集成到 CI 中了。 美中不足的是由于 docker 目前仅支持 linux 上的容器技术，因此它要在非 Linux 系统下运行必须加多一个虚拟机层。这会造成一些在 Linux 上运行不会出现的问题（ip 地址、硬件资源、文件映射等），同时由于基于 linux ，一些 windows 的程序会水土不服，泛用性比虚拟机差一些。 但带来的好处是占用的系统资源低很多。一个只能开数个虚拟机的电脑一般能开数十个 container ，且 container 的启动时间一般在数秒内，比虚拟机快得多。另外，由于 docker 的 image 除了一些特殊的基础镜像外基本都是增量镜像，因此重复部分不会耗费额外的资源，所以几个看起来有数 g 的 image 如果里面使用的基础镜像有重复部分（大部分情况下都会有部分重复），那么它们实际占用空间将会小得多。","categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"https://www.wqp0010.top/tags/docker/"}]},{"title":"SpringBoot2.x整合druid和mybatis-plus","slug":"SpringBoot2-x整合druid和mybatis-plus","date":"2018-12-29T14:56:57.000Z","updated":"2020-06-30T00:33:58.866Z","comments":true,"path":"2018/12/29/SpringBoot2-x整合druid和mybatis-plus/","link":"","permalink":"https://www.wqp0010.top/2018/12/29/SpringBoot2-x整合druid和mybatis-plus/","excerpt":"","text":"前言本篇说一下SpringBoot如何整合druid作为连接池，以及用mybatis-plus做持久层。用到的版本号如下 名称 版本号 SpringBoot 2.1.1.RELEASE druid-spring-boot-starter 1.1.10 mybatis-plus-boot-starter 3.0.6 例子里使用一个user表，直接利用mybatis-plus提供的接口进行增删改查操作 CREATE TABLE `user` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT '主键ID', `name` varchar(30) DEFAULT NULL COMMENT '姓名', `age` int(11) DEFAULT NULL COMMENT '年龄', `email` varchar(50) DEFAULT NULL COMMENT '邮箱', `phone` varchar(45) DEFAULT NULL, `userEvaluation` varchar(45) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin 添加依赖首先，在pom.xml添加以下的依赖 &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.10&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.0.6&lt;/version&gt;&lt;/dependency&gt; 添加相关配置在application.yml中增加以下配置 #数据库配置spring: datasource: druid: #连接信息 url: jdbc:mysql://localhost:3306/test username: root password: 123456 driver-class-name: com.mysql.cj.jdbc.Driver #连接池配置 min-idle: 5 initial-size: 5 max-active: 20 # 配置获取连接等待超时的时间 max-wait: 60000 # 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒 time-between-eviction-runs-millis: 60000 # 配置一个连接在池中最小生存的时间，单位是毫秒 min-evictable-idle-time-millis: 30000 validation-query: SELECT 1 FROM DUAL test-while-idle: true test-on-borrow: false test-on-return: false #监控 filter: wall: enabled: true#mybatis-plus配置mybatis-plus: #xml地址 mapper-locations: - classpath:mapper/*.xml #实体扫描 type-aliases-package: top.wqp0010.s1.demo.entity.UserEntity 代码准备工作完毕，开始写代码。总体结构如下 代码结构 映射xml在resources/mapper里新增一个UserMapper.xml &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"top.wqp0010.s1.demo.mapper.UserMapper\"&gt;&lt;/mapper&gt; 这里并没有写实际的方法，本例我们主要用mybatis-plus提供的方法进行操作，后续再加自己的方法进来 配置类新增一个mybatis-plus的配置类，指定要扫描的包以及定义分页插件 @Configuration@MapperScan(\"top.wqp0010.s1.demo.mapper*\")public class MybatisPlusConfig &#123; /** * mybatis-plus分页插件&lt;br&gt; * 文档：http://mp.baomidou.com&lt;br&gt; */ @Bean public PaginationInterceptor paginationInterceptor() &#123; PaginationInterceptor paginationInterceptor = new PaginationInterceptor(); return paginationInterceptor; &#125;&#125; 新增一个druid的配置类 @Configurationpublic class DruidConfig &#123; private static final Logger log = LoggerFactory.getLogger(DruidConfig.class); @Bean public ServletRegistrationBean druidServlet() &#123; log.info(\"init Druid Servlet Configuration \"); ServletRegistrationBean servletRegistrationBean = new ServletRegistrationBean(); servletRegistrationBean.setServlet(new StatViewServlet()); servletRegistrationBean.addUrlMappings(\"/druid/*\"); Map&lt;String, String&gt; initParameters = new HashMap&lt;String, String&gt;(); initParameters.put(\"loginUsername\", \"admin\");// 用户名 initParameters.put(\"loginPassword\", \"admin\");// 密码 initParameters.put(\"resetEnable\", \"false\");// 禁用HTML页面上的“Reset All”功能 initParameters.put(\"allow\", \"\"); // IP白名单 (没有配置或者为空，则允许所有访问) //initParameters.put(\"deny\", \"192.168.20.38\");// IP黑名单 (存在共同时，deny优先于allow) servletRegistrationBean.setInitParameters(initParameters); return servletRegistrationBean; &#125; @Bean public FilterRegistrationBean filterRegistrationBean() &#123; FilterRegistrationBean filterRegistrationBean = new FilterRegistrationBean(); filterRegistrationBean.setFilter(new WebStatFilter()); filterRegistrationBean.addUrlPatterns(\"/*\"); filterRegistrationBean.addInitParameter(\"exclusions\", \"*.js,*.gif,*.jpg,*.png,*.css,*.ico,/druid/*\"); return filterRegistrationBean; &#125;&#125; 实体类定义UserEntity，这个类和数据库user表对应。这里同时展示了如何映射表名以及表字段和类字段名不同时的处理 /** * 数据库表名为\"user\"，用@TableName映射 */@TableName(\"user\")@Datapublic class UserEntity &#123; /** * 表主键 此处需要设置为数据库ID自增 */ @TableId(type = IdType.AUTO) private long id; private String name; private int age; private String email; private String phone; /** * 此处故意用了类字段名和数据库列明不相符 * 可以用@TableField注解来表示 */ @TableField(\"userEvaluation\") private String evaluation;&#125; Mapper类定义UserMapper接口，主要是继承了mybatis-plus的BaseMapper接口，没有定义自己的方法 public interface UserMapper extends BaseMapper&lt;UserEntity&gt; &#123;&#125; Service接口定义IUserService接口，主要是继承mybatis-plus的IService接口，同样没有自己的实现 public interface IUserService extends IService&lt;UserEntity&gt; &#123;&#125; UserServiceImpl 实现类 @Servicepublic class UserServiceImpl extends ServiceImpl&lt;UserMapper, UserEntity&gt; implements IUserService &#123;&#125; Controller可以看到，上面我们定义的Mapper和Service都是空的，并没有实现具体的方法。现在我们利用mybatis-plus预置的接口实现增删改查 增@RestController@Slf4jpublic class UserController &#123; @Autowired IUserService userService; @RequestMapping(\"/user/add\") public String addUser() &#123; //这里为了测试方便 直接产生了一些随机字符串作为用户参数 UserEntity userEntity = new UserEntity(); userEntity.setName(RandomStringUtils.randomAlphabetic(5)); userEntity.setAge(new Random().nextInt(100)); userEntity.setEmail(RandomStringUtils.randomAlphanumeric(5) + \"@test.com\"); userEntity.setPhone(\"18\" + RandomStringUtils.randomNumeric(9)); userEntity.setEvaluation(RandomStringUtils.randomAlphanumeric(20)); userService.save(userEntity); return \"success insert user = \" + JSON.toJSONString(userEntity);&#125; 删@RequestMapping(\"/user/remove\")public String removeUser(@RequestParam(\"id\") int id) &#123; userService.removeById(id); return \"success delete userId = \" + id;&#125; 改@RequestMapping(\"/user/update\") public String updateUser(@RequestParam(\"id\") int id) &#123; //为了测试方便，属性值都再次随机赋值 UserEntity userEntity = new UserEntity(); userEntity.setId(id); userEntity.setName(RandomStringUtils.randomAlphabetic(5)); userEntity.setAge(new Random().nextInt(100)); userEntity.setEmail(RandomStringUtils.randomAlphanumeric(5) + \"@test.com\"); userEntity.setPhone(\"18\" + RandomStringUtils.randomNumeric(9)); userEntity.setEvaluation(RandomStringUtils.randomAlphanumeric(20)); userService.updateById(userEntity); return \"success update user = \" + JSON.toJSONString(userEntity); &#125; 查单条记录@RequestMapping(\"/user/info\")public UserEntity getUser(@RequestParam(\"id\") int id) &#123; UserEntity userEntity = userService.getById(id); return userEntity;&#125; 分页查询@RequestMapping(\"/user/list\")public IPage getUserList(Page page) &#123; page.setDesc(\"name\"); IPage iPage = userService.page(page); return iPage;&#125; 浏览器访问http://localhost:8080/user/list?size=3&amp;current=1返回的结果即是根据数据库name字段倒叙的分页结果，是不是很方便 打开http://localhost:8080/druid/即可看到druid的监控页 参考 druid官网 mybatis-plus官网","categories":[],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://www.wqp0010.top/tags/SpringBoot/"},{"name":"Druid","slug":"Druid","permalink":"https://www.wqp0010.top/tags/Druid/"},{"name":"Mybatis-Plus","slug":"Mybatis-Plus","permalink":"https://www.wqp0010.top/tags/Mybatis-Plus/"}]},{"title":"SpringBoot2.0整合Prometheus Grafana(二)","slug":"SpringBoot2-0整合Prometheus-Grafana-二","date":"2018-12-28T14:37:08.000Z","updated":"2020-06-30T00:33:58.858Z","comments":true,"path":"2018/12/28/SpringBoot2-0整合Prometheus-Grafana-二/","link":"","permalink":"https://www.wqp0010.top/2018/12/28/SpringBoot2-0整合Prometheus-Grafana-二/","excerpt":"","text":"前言在前一篇文章中，最后能看到的只是Prometheus自带的一些监控指标，本篇就讲一下如何添加自己的监控信息。 增加配置类package com.example.demo.config;import io.micrometer.core.instrument.MeterRegistry;import org.springframework.boot.actuate.autoconfigure.metrics.MeterRegistryCustomizer;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;@Configurationpublic class MetricsConfig &#123; @Bean public MeterRegistryCustomizer&lt;MeterRegistry&gt; commonTags() &#123; return r -&gt; &#123; r.config().commonTags(\"application\", \"com.example.demo.config\"); &#125;; &#125;&#125; CounterCounter是一种只加不减的计数器 java代码测试代码 @RestControllerpublic class TestPrometheusController &#123; static final Counter counter = Metrics.counter(\"getUserInfoCount\", \"user\", \"info\"); @RequestMapping(value = \"/user/info\", method = RequestMethod.GET) public String getInfo() &#123; counter.increment(1D); return \"success\"; &#125;&#125; 程序起来后，打开http://localhost:8080/actuator/prometheus可以看到 # HELP getUserInfoCount_total # TYPE getUserInfoCount_total countergetUserInfoCount_total&#123;application=\"com.example.demo.config\",user=\"info\",&#125; 0.0 这里就可以看到添加项的名称 类型 数量值等信息。浏览器刷新几次http://localhost:8080/user/info后可以观察到这个数量的变化 增加监控项此时如果在打开http://localhost:9090页面，则可以看到多了getUserInfoCount_total监控这一项打开http://localhost:3000 grafana页面，新增表盘时监控项选择“getUserInfoCount_total”即可 GaugeGauge是一个表示单个数值的度量，它可以表示任意地上下移动的数值测量 /** * 假设用来监控用户缓存数量 */AtomicInteger atomicInteger = Metrics.gauge(\"getUserCacheCount\", new AtomicInteger(0));@RequestMapping(value = \"/user/cache/add\", method = RequestMethod.GET)public String addCache() &#123; atomicInteger.addAndGet(1); return \"success\";&#125;@RequestMapping(value = \"/user/cache/remove\", method = RequestMethod.GET)public String removeCache() &#123; atomicInteger.decrementAndGet(); return \"success\";&#125; 启动后，反复调用几次http://localhost:8080/user/cache/add和http://localhost:8080/user/cache/remove,然后参照上面的在grafana添加表盘即可 TimerTimer(计时器)同时测量一个特定的代码逻辑块的调用(执行)速度和它的时间分布 java代码Timer timer = Metrics.timer(\"getUserInfoList\", \"user\", \"list\");@RequestMapping(value = \"/user/list\", method = RequestMethod.GET)public String getList() &#123; long start = System.currentTimeMillis(); try &#123; //模拟耗时操作 TimeUnit.MILLISECONDS.sleep(new Random().nextInt(100) + 100); &#125;catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; long end = System.currentTimeMillis(); long duration = end - start; timer.record(duration, TimeUnit.MILLISECONDS); &#125; return \"success\";&#125; 启动后，反复调用几次http://localhost:8080/user/list 增加监控项添加Timer后，主要是增加了 getUserInfoList_seconds_count&#123;application=\"com.example.demo.config\",user=\"list\",&#125; 12.0getUserInfoList_seconds_sum&#123;application=\"com.example.demo.config\",user=\"list\",&#125; 1.828 在grafana添加仪表选择需要监控的指标时，需要特别配置。前面都是直接添加了已有项，现在要加一些函数操作 平均延时 这样就是平均延时了吞吐量则是 rate(getUserInfoList_seconds_count[10s]) 此处填写时会有模糊匹配函数支持，很方便","categories":[],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://www.wqp0010.top/tags/SpringBoot/"},{"name":"Prometheus","slug":"Prometheus","permalink":"https://www.wqp0010.top/tags/Prometheus/"},{"name":"Grafana","slug":"Grafana","permalink":"https://www.wqp0010.top/tags/Grafana/"}]},{"title":"SpringBoot2.0整合Prometheus Grafana","slug":"SpringBoot2-0整合Prometheus-Grafana","date":"2018-12-27T17:17:11.000Z","updated":"2020-06-30T00:33:58.862Z","comments":true,"path":"2018/12/27/SpringBoot2-0整合Prometheus-Grafana/","link":"","permalink":"https://www.wqp0010.top/2018/12/27/SpringBoot2-0整合Prometheus-Grafana/","excerpt":"","text":"前言总体框架图 Prometheus首页 prometheus简介Prometheus，是一个开源的系统监控和告警的工具包，其采用Pull方式采集时间序列的度量数据（也支持push方式），通过Http协议传输。它的工作方式是被监控的服务需要公开一个Prometheus端点，这端点是一个HTTP接口，该接口公开了度量的列表和当前的值，然后Prometheus应用从此接口定时拉取数据，一般可以存放在时序数据库中，然后通过可视化的Dashboard(e.g.Grafana)进行数据展示。 支持的prometheus metricsCounter，Gauge，Histogram，Summary等等。需要注意的是counter只能增不能减，适用于服务请求量，用户访问数等统计，但是如果需要统计有增有减的指标需要用Gauge。 exporter支持的exporter很多，可以方便的监控很多应用，同时也可以自定义开发非官方提供的exporter。 grafana简介grafana，是一个开源的dashboard展示工具，可以支持很多主流数据源，包括时序性的和非时序性的。其提供的展示配置以及可扩展性能满足绝大部分时间序列数据展示需求，是一个比较优秀的工具。 支持的数据源prometheus，inflexdb，elasticsearch，mysql，postgreSQL，openTSDB等，更多数据源 SpringBoot暴露指标添加依赖在pom.xml增加以下依赖 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.micrometer&lt;/groupId&gt; &lt;artifactId&gt;micrometer-registry-prometheus&lt;/artifactId&gt;&lt;/dependency&gt; 配置信息在application.yml增加以下配置项 ##prometheusjmanagement: metrics: export: prometheus: enabled: true step: 1m descriptions: true web: server: auto-time-requests: true endpoints: prometheus: id: springmetrics web: exposure: include: health,info,env,prometheus,metrics,httptrace,threaddump,heapdump,springmetrics 启动SpringBoot应用启动应用后，打开 http://localhost:8080/actuator/prometheus 即可看到暴露的信息 安装Prometheus官网下载压缩包官网地址：https://prometheus.io/download/根据操作系统下载对应的安装包 解压# tar zxvf prometheus-2.6.0.darwin-amd64.tar.gz 修改配置文件# vim prometheus.yml 修改为以下内容 # my global configglobal: scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute. evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute. # scrape_timeout is set to the global default (10s).# Alertmanager configurationalerting: alertmanagers: - static_configs: - targets: # - alertmanager:9093# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.rule_files: # - \"first_rules.yml\" # - \"second_rules.yml\"# A scrape configuration containing exactly one endpoint to scrape:# Here it's Prometheus itself.scrape_configs: # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config. - job_name: 'prometheus' # metrics_path defaults to '/metrics' # scheme defaults to 'http'. #暴露路径 metrics_path: /actuator/prometheus static_configs: #SpringBoot的ip和端口号 - targets: ['localhost:8080'] 启动# ./prometheus 打开 http://localhost:9090 即可看到页面 Prometheus首页 安装GrafanaMAC平台安装mac需要提前安装brew # brew install grafana 启动# brew services start grafana 配置登陆打开页面http://localhost:3000/login登录账号 admin 密码 admin 增加数据源 增加data source 点击“Add data source” 增加data source 在这里Type选择PrometheusURL填上面的Prometheus访问地址 增加图表 增加dashboard 添加一个Graph 配置panel 配置参数 其他平台其他平台参照http://docs.grafana.org/installation","categories":[],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://www.wqp0010.top/tags/SpringBoot/"},{"name":"Prometheus","slug":"Prometheus","permalink":"https://www.wqp0010.top/tags/Prometheus/"},{"name":"Grafana","slug":"Grafana","permalink":"https://www.wqp0010.top/tags/Grafana/"}]},{"title":"maven打包插件使用","slug":"maven打包插件使用","date":"2018-12-18T16:21:35.000Z","updated":"2020-06-30T00:33:58.866Z","comments":true,"path":"2018/12/18/maven打包插件使用/","link":"","permalink":"https://www.wqp0010.top/2018/12/18/maven打包插件使用/","excerpt":"","text":"背景项目中需要把SpringBoot编译的jar，application.properties，application-pro.properties打包成zip发布 解决方案主要是使用maven-assembly-plugin插件解决 配置pom.xml增加以下内容 &lt;!-- 打成zip包 --&gt;&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;version&gt;2.2.1&lt;/version&gt; &lt;configuration&gt; &lt;descriptors&gt; &lt;!-- 打包配置文件 --&gt; &lt;descriptor&gt;src/main/resources/package.xml&lt;/descriptor&gt; &lt;/descriptors&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;!-- 绑定到这个生命周期 --&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;!--执行一次--&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt; package.xml内容如下 &lt;assembly&gt; &lt;!--这个id会出现在zip包名称的后面，zip的完整名是：pom.xml中的artifactId-version-id.zip 该字段不能为空--&gt; &lt;id&gt;1.1.0&lt;/id&gt; &lt;formats&gt; &lt;!--支持的打包格式有zip、tar、tar.gz (or tgz)、tar.bz2 (or tbz2)、jar、dir、war--&gt; &lt;format&gt;zip&lt;/format&gt; &lt;/formats&gt; &lt;includeBaseDirectory&gt;true&lt;/includeBaseDirectory&gt; &lt;fileSets&gt; &lt;!--管理一组文件的存放位置--&gt; &lt;fileSet&gt; &lt;!-- 打包后在压缩文件中位置，/表示直接放在了压缩文件根目录 --&gt; &lt;outputDirectory&gt;/&lt;/outputDirectory&gt; &lt;directory&gt;src/main/resources/&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;application.properties&lt;/include&gt; &lt;include&gt;application-prod.properties&lt;/include&gt; &lt;/includes&gt; &lt;/fileSet&gt; &lt;/fileSets&gt; &lt;files&gt; &lt;file&gt; &lt;!-- 一个文件 --&gt; &lt;source&gt;target/test.jar&lt;/source&gt; &lt;!-- 源文件 --&gt; &lt;outputDirectory&gt;/&lt;/outputDirectory&gt; &lt;destName&gt;application.jar&lt;/destName&gt; &lt;!-- 目标文件名称--&gt; &lt;/file&gt; &lt;/files&gt;&lt;/assembly&gt; 详细配置详细配置见官网 http://maven.apache.org/plugins/maven-assembly-plugin/assembly.html","categories":[],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://www.wqp0010.top/tags/SpringBoot/"},{"name":"maven","slug":"maven","permalink":"https://www.wqp0010.top/tags/maven/"},{"name":"pom","slug":"pom","permalink":"https://www.wqp0010.top/tags/pom/"}]},{"title":"SpringBoot前后端整合","slug":"SpringBoot前后端整合","date":"2018-12-18T15:31:23.000Z","updated":"2020-06-30T00:33:58.866Z","comments":true,"path":"2018/12/18/SpringBoot前后端整合/","link":"","permalink":"https://www.wqp0010.top/2018/12/18/SpringBoot前后端整合/","excerpt":"","text":"背景项目中后台代码用的SpringBoot，前端代码用的Vue，现在想放到一个项目中 用maven管理 代码结构总结构大概如下： all backend - pom.xml frontend- pom.xml pom.xml 思路 编译前端代码 复制编译好的前端文件到后台代码 编译后台代码 pom主工程&lt;modules&gt; &lt;module&gt;frontend&lt;/module&gt; &lt;module&gt;backend&lt;/module&gt;&lt;/modules&gt; 主要是写好模块 指定顺序 frontend&lt;build&gt; &lt;plugins&gt; &lt;!-- 配置npm安装 --&gt; &lt;plugin&gt; &lt;groupId&gt;com.github.eirslett&lt;/groupId&gt; &lt;artifactId&gt;frontend-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.6&lt;/version&gt; &lt;executions&gt; &lt;!-- 安装nodejs和npm--&gt; &lt;execution&gt; &lt;id&gt;install node and npm&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;install-node-and-npm&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;nodeVersion&gt;v10.12.0&lt;/nodeVersion&gt; &lt;npmVersion&gt;6.4.1&lt;/npmVersion&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;!-- 安装webpack --&gt; &lt;execution&gt; &lt;id&gt;npm install webpack&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;npm&lt;/goal&gt; &lt;/goals&gt; &lt;phase&gt;generate-resources&lt;/phase&gt; &lt;configuration&gt; &lt;arguments&gt;install webpack -g&lt;/arguments&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;!-- 安装项目依赖 --&gt; &lt;execution&gt; &lt;id&gt;npm install&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;npm&lt;/goal&gt; &lt;/goals&gt; &lt;phase&gt;generate-resources&lt;/phase&gt; &lt;configuration&gt; &lt;arguments&gt;install&lt;/arguments&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;!-- 编译 --&gt; &lt;execution&gt; &lt;id&gt;npm run build&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;npm&lt;/goal&gt; &lt;/goals&gt; &lt;phase&gt;generate-resources&lt;/phase&gt; &lt;configuration&gt; &lt;arguments&gt;run build&lt;/arguments&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 主要是利用frontend-maven-plugin插件进行一系列的操作: 1.安装nodejs和npm命令 2.安装webpack 3.安装项目依赖 4.编译项目 backend&lt;build&gt; &lt;plugins&gt; &lt;!-- 复制前端编译好的页面 --&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;copy frontend content&lt;/id&gt; &lt;phase&gt;generate-resources&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;copy-resources&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;!-- 复制文件的目的目录 --&gt; &lt;outputDirectory&gt;src/main/resources/static/&lt;/outputDirectory&gt; &lt;overwrite&gt;true&lt;/overwrite&gt; &lt;resources&gt; &lt;resource&gt; &lt;!-- 复制文件的源目录 --&gt; &lt;directory&gt;$&#123;project.parent.basedir&#125;/frontend/target/dist&lt;/directory&gt; &lt;filtering&gt;false&lt;/filtering&gt; &lt;includes&gt; &lt;include&gt;**/*&lt;/include&gt; &lt;/includes&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 主要是利用frontend-maven-plugin插件,将编译好的前端文件复制到后台代码指定目录中 结果至此，即可完成需求根目录下执行mvn package即可编译完整项目","categories":[],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://www.wqp0010.top/tags/SpringBoot/"},{"name":"maven","slug":"maven","permalink":"https://www.wqp0010.top/tags/maven/"},{"name":"pom","slug":"pom","permalink":"https://www.wqp0010.top/tags/pom/"},{"name":"Vue","slug":"Vue","permalink":"https://www.wqp0010.top/tags/Vue/"}]},{"title":"docker命令 (二)","slug":"docker命令（二）","date":"2018-12-18T14:30:03.000Z","updated":"2020-06-30T00:33:58.866Z","comments":true,"path":"2018/12/18/docker命令（二）/","link":"","permalink":"https://www.wqp0010.top/2018/12/18/docker命令（二）/","excerpt":"","text":"镜像使用列出镜像列表# docker images 显示： REPOSITORY TAG IMAGE ID CREATED SIZEcentos 6.7 192ad0341c8b 2 months ago 191MBopenjdk 8-jdk-alpine 54ae553cb104 3 months ago 103MBtraining/webapp latest 6fae60ef3446 3 years ago 349MB 说明： REPOSITORY：表示镜像的仓库源 TAG：镜像的标签 IMAGE ID：镜像ID CREATED：镜像创建时间 SIZE：镜像大小 同一仓库源可以有多个 TAG，代表这个仓库源的不同个版本，如ubuntu仓库源里，有15.10、14.04等多个不同的版本，我们使用 REPOSITORY:TAG 来定义不同的镜像。 获取镜像# docker pull centos:6.7 下载完成后，我们可以直接使用这个镜像来运行容器。 查找镜像# docker search httpd 显示为： NAME DESCRIPTION STARS OFFICIAL AUTOMATEDhttpd The Apache HTTP Server Project 2236 [OK]hypriot/rpi-busybox-httpd Raspberry Pi compatible Docker Image with a … 45centos/httpd 21 [OK]centos/httpd-24-centos7 Platform for running Apache httpd 2.4 or bui… 20 说明： NAME:镜像仓库源的名称 DESCRIPTION:镜像的描述 OFFICIAL:是否docker官方发布 创建镜像更新镜像创建容器# docker run -t -i ubuntu:15.10 /bin/bash 在容器内执行apt-get update 然后退出 提交容器# docker commit -m=\"has update\" -a=\"wu\" e218edb10161 wu/ubuntu:v2 各个参数说明： -m:提交的描述信息 -a:指定镜像作者 e218edb10161：容器ID wu/ubuntu:v2:指定要创建的目标镜像名 构建镜像创建Dockerfile# vim Dockerfile 内容如下: FROM centos:6.7MAINTAINER Fisher \"fisher@sudops.com\"RUN /bin/echo 'root:123456' |chpasswdRUN useradd wuRUN /bin/echo 'wu:123456' |chpasswdRUN /bin/echo -e \"LANG=\\\"en_US.UTF-8\\\"\" &gt;/etc/default/localEXPOSE 22EXPOSE 80CMD /usr/sbin/sshd -D 每一个指令都会在镜像上创建一个新的层，每一个指令的前缀都必须是大写的。第一条FROM，指定使用哪个镜像源RUN 指令告诉docker 在镜像内执行命令，安装了什么。。。 构建镜像# docker build -t wu/centos:6.7 . 说明： -t ：指定要创建的目标镜像名 . ：Dockerfile 文件所在目录，可以指定Dockerfile 的绝对路径 启动构造好的镜像# docker run -t -i wu/centos:6.7 /bin/bash 在镜像内运行： [root@e1b7c2d96228 /]# id wuuid=500(wu) gid=500(wu) groups=500(wu) 可以发现已经成功创建了用户wu 设置镜像标签# docker tag 286139a0cf69 wu/centos:dev 可以发现镜像中多了一条Tag为dev的记录","categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"https://www.wqp0010.top/tags/docker/"}]},{"title":"docker命令 (一)","slug":"docker命令 (一)","date":"2018-12-17T11:33:07.000Z","updated":"2020-06-30T00:33:58.866Z","comments":true,"path":"2018/12/17/docker命令 (一)/","link":"","permalink":"https://www.wqp0010.top/2018/12/17/docker命令 (一)/","excerpt":"","text":"启动docker# docker run centos:6.7 /bin/echo \"Hello world\" 结果： Hello world 说明： docker run 运行一个容器 centos:6.7 指定要运行的镜像，Docker首先从本地主机上查找镜像是否存在，如果不存在，Docker 就会从镜像仓库 Docker Hub 下载公共镜像 /bin/echo “Hello world” 在启动的容器里执行的命令 运行交互式的容器# docker run -i -t centos:6.7 /bin/bash 结果： [root@95f68434a3b2 /]# 说明： -t 在新容器内指定一个伪终端或终端 -i 允许你对容器内的标准输入 (STDIN) 进行交互 启动容器后台启动# docker run -d centos:6.7 /bin/sh -c \"while true; do echo hello world; sleep 1; done\" 结果： 4767f1d1c05be0967e68a61418a40a1c538dc4b7c362b71aafb8c79dcbffc4c3 这个长字符串叫做容器ID，对每个容器来说都是唯一的 查看后台运行的容器# docker ps 显示： CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES4767f1d1c05b centos:6.7 \"/bin/sh -c 'while t…\" 10 seconds ago Up 9 seconds compassionate_perlman 说明： CONTAINER ID:容器ID NAMES:自动分配的容器名称 查看容器内的输出# docker logs 4767f1d1c05b 显示： hello worldhello world... 说明：CONTAINER ID和NAMES都可以生效 停止容器# docker stop 4767f1d1c05b 再次用docker ps查看时发现已经停止了 docker容器运行一个web应用拉取镜像# docker pull training/webapp 启动web应用# docker run -d -P training/webapp python app.py 参数说明: -d:让容器在后台运行。 -P:将容器内部使用的网络端口映射到我们使用的主机上。 查看应用#docker ps 显示： CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESe8052c72934f training/webapp \"python app.py\" 8 seconds ago Up 6 seconds 0.0.0.0:32768-&gt;5000/tcp fervent_hoover PORTS列的0.0.0.0:32768-&gt;5000/tcp表示docker开放了5000端口映射到主机32768端口通过 http://localhost:32768 即可打开页面 设置端口# docker run -d -p 5000:5000 training/webapp python app.py 则可以吧docker的5000端口映射到主机的5000端口 查看端口映射# docker port 9602135d4352``` 则显示：``` bash5000/tcp -&gt; 0.0.0.0:5000 查看日志# docker logs -f 9602135d4352 -f: 让 docker logs 像使用 tail -f 一样来输出容器内部的标准输出。 移除容器# docker rm 9602135d4352 删除容器时，容器必须是停止状态","categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"https://www.wqp0010.top/tags/docker/"}]},{"title":"JVM调优","slug":"JVM调优","date":"2018-11-22T10:49:48.000Z","updated":"2020-06-30T00:33:58.858Z","comments":true,"path":"2018/11/22/JVM调优/","link":"","permalink":"https://www.wqp0010.top/2018/11/22/JVM调优/","excerpt":"","text":"查看java进程的启动参数# jcmd 27876 VM.flags 得到以下结果 27876:-XX:CICompilerCount=3 -XX:InitialHeapSize=264241152 -XX:MaxHeapSize=4208984064 -XX:MaxNewSize=1402994688 -XX:MinHeapDeltaBytes=524288 -XX:NewSize=88080384 -XX:OldSize=176160768 -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseParallelGC 打印堆概要信息# jmap -heap 27876 得到以下结果 Attaching to process ID 27876, please wait...Debugger attached successfully.Server compiler detected.JVM version is 25.171-b11using thread-local object allocation.Parallel GC with 4 thread(s)Heap Configuration: MinHeapFreeRatio = 0 MaxHeapFreeRatio = 100 MaxHeapSize = 4208984064 (4014.0MB) NewSize = 88080384 (84.0MB) MaxNewSize = 1402994688 (1338.0MB) OldSize = 176160768 (168.0MB) NewRatio = 2 SurvivorRatio = 8 MetaspaceSize = 21807104 (20.796875MB) CompressedClassSpaceSize = 1073741824 (1024.0MB) MaxMetaspaceSize = 17592186044415 MB G1HeapRegionSize = 0 (0.0MB)Heap Usage:PS Young GenerationEden Space: capacity = 706215936 (673.5MB) used = 53656856 (51.171165466308594MB) free = 652559080 (622.3288345336914MB) 7.597797396630823% usedFrom Space: capacity = 3670016 (3.5MB) used = 0 (0.0MB) free = 3670016 (3.5MB) 0.0% usedTo Space: capacity = 20447232 (19.5MB) used = 0 (0.0MB) free = 20447232 (19.5MB) 0.0% usedPS Old Generation capacity = 282066944 (269.0MB) used = 40132768 (38.273590087890625MB) free = 241934176 (230.72640991210938MB) 14.228100404420307% used26205 interned Strings occupying 3370072 bytes. 查看GC情况# jstat -gccause 27876 5000 100 参数说明： 字段 说明 gccause 垃圾收集统计概述（同-gcutil），附加最近两次垃圾回收事件的原因。 5000 打印间隔 100 打印次数 得到如下结果： S0 S1 E O M CCS YGC YGCT FGC FGCT GCT LGCC GCC0.00 0.00 7.60 14.23 97.49 96.72 16 0.312 4 0.636 0.948 Heap Dump Initiated GC No GC0.00 0.00 7.60 14.23 97.49 96.72 16 0.312 4 0.636 0.948 Heap Dump Initiated GC No GC0.00 0.00 7.60 14.23 97.49 96.72 16 0.312 4 0.636 0.948 Heap Dump Initiated GC No GC0.00 0.00 7.60 14.23 97.49 96.72 16 0.312 4 0.636 0.948 Heap Dump Initiated GC No GC 获取堆信息# jmap -dump:live,format=b,file=test.hprof 27876 得到一个test.hprof文件，用MAT打开就可以看到堆信息 查看线程信息# jstack -l 2826|more 参数说明： 字段 说明 F 当正常输出请求不被响应时，强制输出线程堆栈 l 除堆栈外，显示关于锁的附加信息 m 如果调用到本地方法的话，可以显示C/C++的堆栈 查看非堆内存情况需要在启动时加参数-XX:NativeMemoryTracking=summary -XX:+UnlockDiagnosticVMOptions -XX:+PrintNMTStatistics注意该参数会导致一定的性能损耗，生产环境不建议使用 java -XX:NativeMemoryTracking=summary -XX:+UnlockDiagnosticVMOptions -XX:+PrintNMTStatistics -jar app.jar 查看非堆内存 jcmd 2826 VM.native_memory summary scale=MB","categories":[],"tags":[{"name":"JVM","slug":"JVM","permalink":"https://www.wqp0010.top/tags/JVM/"}]}]}